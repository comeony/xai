diff --git a/lime/discretize.py b/lime/discretize.py
index df3df0a..bfe9f47 100644
--- a/lime/discretize.py
+++ b/lime/discretize.py
@@ -47,18 +47,18 @@ class BaseDiscretizer():
         self.maxs = {}
         self.random_state = check_random_state(random_state)
 
-        # To override when implementing a custom binning
-        bins = self.bins(data, labels)
-        bins = [np.unique(x) for x in bins]
-
         # Read the stats from data_stats if exists
         if data_stats:
             self.means = self.data_stats.get("means")
             self.stds = self.data_stats.get("stds")
             self.mins = self.data_stats.get("mins")
             self.maxs = self.data_stats.get("maxs")
+            self.bins_value = self.data_stats.get("bins")
+        else:
+            self.bins_value = self.bins(data, labels)
+            self.bins_value = [np.unique(x) for x in self.bins_value]
 
-        for feature, qts in zip(self.to_discretize, bins):
+        for feature, qts in zip(self.to_discretize, self.bins_value):
             n_bins = qts.shape[0]  # Actually number of borders (= #bins-1)
             boundaries = np.min(data[:, feature]), np.max(data[:, feature])
             name = feature_names[feature]
@@ -174,7 +174,6 @@ class StatsDiscretizer(BaseDiscretizer):
 
 class QuartileDiscretizer(BaseDiscretizer):
     def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):
-
         BaseDiscretizer.__init__(self, data, categorical_features,
                                  feature_names, labels=labels,
                                  random_state=random_state)
@@ -204,7 +203,7 @@ class DecileDiscretizer(BaseDiscretizer):
 
 class EntropyDiscretizer(BaseDiscretizer):
     def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):
-        if(labels is None):
+        if (labels is None):
             raise ValueError('Labels must be not None when using \
                              EntropyDiscretizer')
         BaseDiscretizer.__init__(self, data, categorical_features,
diff --git a/lime/explanation.py b/lime/explanation.py
index d03155e..92ba2f9 100644
--- a/lime/explanation.py
+++ b/lime/explanation.py
@@ -7,8 +7,7 @@ import os.path
 import json
 import string
 import numpy as np
-
-from .exceptions import LimeError
+import warnings
 
 from sklearn.utils import check_random_state
 
@@ -95,8 +94,8 @@ class Explanation(object):
         self.domain_mapper = domain_mapper
         self.local_exp = {}
         self.intercept = {}
-        self.score = None
-        self.local_pred = None
+        self.score = {}
+        self.local_pred = {}
         if mode == 'classification':
             self.class_names = class_names
             self.top_labels = None
@@ -108,9 +107,9 @@ class Explanation(object):
             self.max_value = 1.0
             self.dummy_label = 1
         else:
-            raise LimeError('Invalid explanation mode "{}". '
-                            'Should be either "classification" '
-                            'or "regression".'.format(mode))
+            raise ValueError('Invalid explanation mode "{}". '
+                             'Should be either "classification" '
+                             'or "regression".'.format(mode))
 
     def available_labels(self):
         """
@@ -139,7 +138,7 @@ class Explanation(object):
         """
         label_to_use = label if self.mode == "classification" else self.dummy_label
         ans = self.domain_mapper.map_exp_ids(self.local_exp[label_to_use], **kwargs)
-        ans = [(x[0], float(x[1])) for x in ans]
+        ans = [(str(x[0]), float(x[1])) for x in ans]
         return ans
 
     def as_map(self):
@@ -150,7 +149,7 @@ class Explanation(object):
         """
         return self.local_exp
 
-    def as_pyplot_figure(self, label=1, **kwargs):
+    def as_pyplot_figure(self, label=1, sample=0, figsize=(4,4), **kwargs):
         """Returns the explanation as a pyplot figure.
 
         Will throw an error if you don't have matplotlib installed
@@ -158,6 +157,8 @@ class Explanation(object):
             label: desired label. If you ask for a label for which an
                    explanation wasn't computed, will throw an exception.
                    Will be ignored for regression explanations.
+            sample: sample index, as part of pyplot title.
+            figsize: desired size of pyplot in tuple format, defaults to (4,4).
             kwargs: keyword arguments, passed to domain_mapper
 
         Returns:
@@ -165,7 +166,7 @@ class Explanation(object):
         """
         import matplotlib.pyplot as plt
         exp = self.as_list(label=label, **kwargs)
-        fig = plt.figure()
+        fig = plt.figure(figsize=figsize)
         vals = [x[1] for x in exp]
         names = [x[0] for x in exp]
         vals.reverse()
@@ -175,13 +176,14 @@ class Explanation(object):
         plt.barh(pos, vals, align='center', color=colors)
         plt.yticks(pos, names)
         if self.mode == "classification":
-            title = 'Local explanation for class %s' % self.class_names[label]
+            title = 'Local explanation for sample %d for class %s' % (sample, self.class_names[label])
         else:
-            title = 'Local explanation'
+            title = 'Local explanation for sample %d' % sample
         plt.title(title)
         return fig
 
     def show_in_notebook(self,
+                         sample_index,
                          labels=None,
                          predict_proba=True,
                          show_predicted_value=True,
@@ -190,9 +192,13 @@ class Explanation(object):
 
         See as_html() for parameters.
         This will throw an error if you don't have IPython installed"""
-
-        from IPython.core.display import display, HTML
-        display(HTML(self.as_html(labels=labels,
+        try:
+            from IPython.core.display import display, HTML
+        except ModuleNotFoundError:
+            warnings.warn("'IPython' not installed, not showing the explanation.")
+            return
+        display(HTML(self.as_html(sample_index,
+                                  labels=labels,
                                   predict_proba=predict_proba,
                                   show_predicted_value=show_predicted_value,
                                   **kwargs)))
@@ -219,6 +225,7 @@ class Explanation(object):
         file_.close()
 
     def as_html(self,
+                sample_index,
                 labels=None,
                 predict_proba=True,
                 show_predicted_value=True,
@@ -226,6 +233,7 @@ class Explanation(object):
         """Returns the explanation as an html page.
 
         Args:
+            sample_index: sample index.
             labels: desired labels to show explanations for (as barcharts).
                 If you ask for a label for which an explanation wasn't
                 computed, will throw an exception. If None, will show
@@ -255,8 +263,9 @@ class Explanation(object):
         <head><script>%s </script></head><body>''' % bundle
         random_id = id_generator(size=15, random_state=check_random_state(self.random_state))
         out += u'''
+        <h3>Sample %d</h3>
         <div class="lime top_div" id="top_div%s"></div>
-        ''' % random_id
+        ''' % (sample_index, random_id)
 
         predict_proba_js = ''
         if self.mode == "classification" and predict_proba:
diff --git a/lime/lime_tabular.py b/lime/lime_tabular.py
index f788315..64bdb0f 100644
--- a/lime/lime_tabular.py
+++ b/lime/lime_tabular.py
@@ -12,14 +12,14 @@ import scipy as sp
 import sklearn
 import sklearn.preprocessing
 from sklearn.utils import check_random_state
+import mindspore as ms
 
-from lime.discretize import QuartileDiscretizer
-from lime.discretize import DecileDiscretizer
-from lime.discretize import EntropyDiscretizer
-from lime.discretize import BaseDiscretizer
-from lime.discretize import StatsDiscretizer
-from . import explanation
-from . import lime_base
+from .discretize import QuartileDiscretizer
+from .discretize import DecileDiscretizer
+from .discretize import EntropyDiscretizer
+from .discretize import BaseDiscretizer
+from .discretize import StatsDiscretizer
+from . import lime_base, explanation
 
 
 class TableDomainMapper(explanation.DomainMapper):
@@ -95,10 +95,10 @@ class TableDomainMapper(explanation.DomainMapper):
                                     fweights))
             else:
                 out_dict = dict(map(lambda x: (x[0], (x[1], x[2], x[3])),
-                                zip(self.feature_indexes,
-                                    fnames,
-                                    self.feature_values,
-                                    fweights)))
+                                    zip(self.feature_indexes,
+                                        fnames,
+                                        self.feature_values,
+                                        fweights)))
                 out_list = [out_dict.get(x[0], (str(x[0]), 0.0, 0.0)) for x in exp]
         else:
             out_list = list(zip(self.exp_feature_names,
@@ -122,8 +122,8 @@ class LimeTabularExplainer(object):
     explained."""
 
     def __init__(self,
+                 model,
                  training_data,
-                 mode="classification",
                  training_labels=None,
                  feature_names=None,
                  categorical_features=None,
@@ -141,8 +141,10 @@ class LimeTabularExplainer(object):
         """Init function.
 
         Args:
+            model (Cell, function): The black-box model to be explained, or a prediction function. For classifiers,
+                the function should take a tensor and outputs prediction probabilities. For regressors, this takes a
+                tensor and returns the predictions.
             training_data: numpy 2d array
-            mode: "classification" or "regression"
             training_labels: labels for training data. Not required, but may be
                 used by discretizer.
             feature_names: list of names (strings) corresponding to the columns
@@ -184,6 +186,28 @@ class LimeTabularExplainer(object):
                 means", "mins", "maxs", "stds", "feature_values",
                 "feature_frequencies"
         """
+        mode = "regression"
+        # check if model accept mindspore.Tensor as input
+        try:
+            model(ms.Tensor(training_data[:1], ms.float32))
+            self._model_is_cell = True
+        except:
+            self._model_is_cell = False
+
+        if self._model_is_cell:
+            inputs = ms.Tensor(training_data[:1], ms.float32)
+        else:
+            inputs = training_data[:1]
+        outputs = model(inputs)
+
+        if len(outputs.shape) > 1 and outputs.shape[1] > 1:
+            mode = "classification"
+
+        if isinstance(training_data, ms.Tensor):
+            training_data = training_data.asnumpy()
+
+        self._model = model
+
         self.random_state = check_random_state(random_state)
         self.mode = mode
         self.categorical_names = categorical_names or {}
@@ -213,19 +237,19 @@ class LimeTabularExplainer(object):
 
             if discretizer == 'quartile':
                 self.discretizer = QuartileDiscretizer(
-                        training_data, self.categorical_features,
-                        self.feature_names, labels=training_labels,
-                        random_state=self.random_state)
+                    training_data, self.categorical_features,
+                    self.feature_names, labels=training_labels,
+                    random_state=self.random_state)
             elif discretizer == 'decile':
                 self.discretizer = DecileDiscretizer(
-                        training_data, self.categorical_features,
-                        self.feature_names, labels=training_labels,
-                        random_state=self.random_state)
+                    training_data, self.categorical_features,
+                    self.feature_names, labels=training_labels,
+                    random_state=self.random_state)
             elif discretizer == 'entropy':
                 self.discretizer = EntropyDiscretizer(
-                        training_data, self.categorical_features,
-                        self.feature_names, labels=training_labels,
-                        random_state=self.random_state)
+                    training_data, self.categorical_features,
+                    self.feature_names, labels=training_labels,
+                    random_state=self.random_state)
             elif isinstance(discretizer, BaseDiscretizer):
                 self.discretizer = discretizer
             else:
@@ -235,7 +259,7 @@ class LimeTabularExplainer(object):
             self.categorical_features = list(range(training_data.shape[1]))
 
             # Get the discretized_training_data when the stats are not provided
-            if(self.training_data_stats is None):
+            if (self.training_data_stats is None):
                 discretized_training_data = self.discretizer.discretize(
                     training_data)
 
@@ -295,7 +319,6 @@ class LimeTabularExplainer(object):
 
     def explain_instance(self,
                          data_row,
-                         predict_fn,
                          labels=(1,),
                          top_labels=None,
                          num_features=10,
@@ -303,22 +326,12 @@ class LimeTabularExplainer(object):
                          distance_metric='euclidean',
                          model_regressor=None):
         """Generates explanations for a prediction.
-
         First, we generate neighborhood data by randomly perturbing features
         from the instance (see __data_inverse). We then learn locally weighted
         linear models on this neighborhood data to explain each of the classes
         in an interpretable way (see lime_base.py).
-
         Args:
             data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row
-            predict_fn: prediction function. For classifiers, this should be a
-                function that takes a numpy array and outputs prediction
-                probabilities. For regressors, this takes a numpy array and
-                returns the predictions. For ScikitClassifiers, this is
-                `classifier.predict_proba()`. For ScikitRegressors, this
-                is `regressor.predict()`. The prediction function needs to work
-                on multiple feature vectors (the vectors randomly perturbed
-                from the data_row).
             labels: iterable with labels to be explained.
             top_labels: if not None, ignore labels and produce explanations for
                 the K labels with highest prediction probabilities, where K is
@@ -329,7 +342,6 @@ class LimeTabularExplainer(object):
             model_regressor: sklearn regressor to use in explanation. Defaults
                 to Ridge regression in LimeBase. Must have model_regressor.coef_
                 and 'sample_weight' as a parameter to model_regressor.fit()
-
         Returns:
             An Explanation object (see explanation.py) with the corresponding
             explanations.
@@ -347,12 +359,17 @@ class LimeTabularExplainer(object):
         else:
             scaled_data = (data - self.scaler.mean_) / self.scaler.scale_
         distances = sklearn.metrics.pairwise_distances(
-                scaled_data,
-                scaled_data[0].reshape(1, -1),
-                metric=distance_metric
+            scaled_data,
+            scaled_data[0].reshape(1, -1),
+            metric=distance_metric
         ).ravel()
 
-        yss = predict_fn(inverse)
+        if self._model_is_cell:
+            yss = self._model(ms.Tensor(inverse, ms.float32))
+        else:
+            yss = self._model(inverse)
+        if isinstance(yss, ms.Tensor):
+            yss = yss.asnumpy()
 
         # for classification, the model needs to provide a list of tuples - classes
         # along with prediction probabilities
@@ -424,7 +441,7 @@ class LimeTabularExplainer(object):
             discretized_feature_names = copy.deepcopy(feature_names)
             for f in self.discretizer.names:
                 discretized_feature_names[f] = self.discretizer.names[f][int(
-                        discretized_instance[f])]
+                    discretized_instance[f])]
 
         domain_mapper = TableDomainMapper(feature_names,
                                           values,
@@ -449,14 +466,15 @@ class LimeTabularExplainer(object):
         for label in labels:
             (ret_exp.intercept[label],
              ret_exp.local_exp[label],
-             ret_exp.score, ret_exp.local_pred) = self.base.explain_instance_with_data(
-                    scaled_data,
-                    yss,
-                    distances,
-                    label,
-                    num_features,
-                    model_regressor=model_regressor,
-                    feature_selection=self.feature_selection)
+             ret_exp.score[label],
+             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(
+                scaled_data,
+                yss,
+                distances,
+                label,
+                num_features,
+                model_regressor=model_regressor,
+                feature_selection=self.feature_selection)
 
         if self.mode == "regression":
             ret_exp.intercept[1] = ret_exp.intercept[0]
@@ -508,9 +526,12 @@ class LimeTabularExplainer(object):
                 instance_sample = data_row[:, non_zero_indexes]
                 scale = scale[non_zero_indexes]
                 mean = mean[non_zero_indexes]
+
             data = self.random_state.normal(
                 0, 1, num_samples * num_cols).reshape(
                 num_samples, num_cols)
+            data = np.array(data)
+
             if self.sample_around_instance:
                 data = data * scale + instance_sample
             else:
@@ -614,7 +635,7 @@ class RecurrentTabularExplainer(LimeTabularExplainer):
         # Reshape X
         n_samples, n_timesteps, n_features = training_data.shape
         training_data = np.transpose(training_data, axes=(0, 2, 1)).reshape(
-                n_samples, n_timesteps * n_features)
+            n_samples, n_timesteps * n_features)
         self.n_timesteps = n_timesteps
         self.n_features = n_features
 
@@ -624,20 +645,20 @@ class RecurrentTabularExplainer(LimeTabularExplainer):
 
         # Send off the the super class to do its magic.
         super(RecurrentTabularExplainer, self).__init__(
-                training_data,
-                mode=mode,
-                training_labels=training_labels,
-                feature_names=feature_names,
-                categorical_features=categorical_features,
-                categorical_names=categorical_names,
-                kernel_width=kernel_width,
-                kernel=kernel,
-                verbose=verbose,
-                class_names=class_names,
-                feature_selection=feature_selection,
-                discretize_continuous=discretize_continuous,
-                discretizer=discretizer,
-                random_state=random_state)
+            training_data,
+            mode=mode,
+            training_labels=training_labels,
+            feature_names=feature_names,
+            categorical_features=categorical_features,
+            categorical_names=categorical_names,
+            kernel_width=kernel_width,
+            kernel=kernel,
+            verbose=verbose,
+            class_names=class_names,
+            feature_selection=feature_selection,
+            discretize_continuous=discretize_continuous,
+            discretizer=discretizer,
+            random_state=random_state)
 
     def _make_predict_proba(self, func):
         """
